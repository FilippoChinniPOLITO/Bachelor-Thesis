import math
import warnings
from typing import Any, Tuple

import torch
from torch import nn, Tensor

from torch.nn import functional as F

from transformers import SegformerModel, SegformerConfig


MIT_SETTINGS = {
    'B0': [[32, 64, 160, 256], [2, 2, 2, 2]],  # [embed_dims, depths]
    'B1': [[64, 128, 320, 512], [2, 2, 2, 2]],
    'B2': [[64, 128, 320, 512], [3, 4, 6, 3]],
    'B3': [[64, 128, 320, 512], [3, 4, 18, 3]],
    'B4': [[64, 128, 320, 512], [3, 8, 27, 3]],
    'B5': [[64, 128, 320, 512], [3, 6, 40, 3]],

    'LD': [[16, 32, 80, 128], [2, 2, 2, 2]],  # Lightweight deep
    # 'LT': [[32, 64, 160, 256], [1, 1, 1, 1]], # Lightweight thick
    'L0': [[16, 32, 80, 128], [1, 1, 1, 1]],
    'L1': [[8, 16, 40, 64], [1, 1, 1, 1]],
    'L2': [[4, 8, 20, 32], [1, 1, 1, 1]],
}


def get_param(dictionary: dict, key: str, default_val=None):
    if key in dictionary:
        return dictionary[key]
    return default_val


class BaseModel(nn.Module):
    def __init__(self, backbone: str = 'MiT-B0', input_channels: int = 3, backbone_pretrained: bool = False) -> None:
        super().__init__()
        self.backbone_pretrained = backbone_pretrained
        self.backbone = self.eval_backbone(backbone, input_channels, pretrained=backbone_pretrained)
        self.encoder_maps_sizes = self.backbone.channels

    def _init_weights(self, m: nn.Module) -> None:
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out // m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                nn.init.zeros_(m.bias)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):
            nn.init.ones_(m.weight)
            nn.init.zeros_(m.bias)

    def init_pretrained(self, pretrained: str = None) -> None:
        if pretrained:
            self.backbone.load_state_dict(torch.load(pretrained, map_location='cpu'), strict=False)

    def initialize_param_groups(self, lr: float, training_params: dict) -> list:
        freeze_pretrained = get_param(training_params, 'freeze_pretrained', False)
        if self.backbone_pretrained and freeze_pretrained:
            return [{'named_params': list(
                filter(lambda x: not x[0].startswith('backbone'), list(self.named_parameters())))}]
        return [{'named_params': self.named_parameters()}]

    @classmethod
    def eval_backbone(cls, backbone: str, input_channels: int, n_blocks: int = 4,
                      pretrained: bool = False) -> nn.Module:
        backbone, variant = backbone.split('-')
        return eval(backbone)(variant, input_channels, n_blocks=n_blocks, pretrained=pretrained)


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


class MiT(nn.Module):
    pretrained_url = ("nvidia/segformer-", "-finetuned-ade-512-512")

    def __init__(self, model_name: str = 'B0', input_channels=3, n_blocks=4, pretrained=False):
        super().__init__()
        if not pretrained:
            assert model_name in MIT_SETTINGS.keys(), f"MiT model name should be in {list(MIT_SETTINGS.keys())}"
            embed_dims, depths = MIT_SETTINGS[model_name]
            drop_path_rate = 0.1
            self.channels = embed_dims

            # patch_embed
            patch_sizes = [7, 3, 3, 3]
            paddings = [4, 2, 2, 2]
            for i in range(n_blocks):
                c_in = input_channels if i == 0 else embed_dims[i - 1]
                setattr(self, f"patch_embed{i + 1}", PatchEmbed(c_in, embed_dims[i], patch_sizes[i], paddings[i]))

            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]

            heads = [1, 2, 5, 8]
            ratios = [8, 4, 2, 1]
            cur = 0
            for i in range(n_blocks):
                setattr(self, f"block{i + 1}",
                        nn.ModuleList(
                            [Block(embed_dims[i], heads[i], ratios[i], dpr[cur + k]) for k in range(depths[i])]))
                setattr(self, f"norm{i + 1}", nn.LayerNorm(embed_dims[i]))
                cur += depths[i]
            self.forward = self.base_forward
            self.partial_forward = self.base_partial_forward
            self.n_blocks = n_blocks
        else:
            url = self.pretrained_url[0] + model_name.lower() + self.pretrained_url[1]
            self.url = url
            config = SegformerConfig().from_pretrained(url)
            config.num_encoder_blocks = n_blocks
            config.num_channels = input_channels
            self.config = config
            self.channels = config.hidden_sizes
            self.encoder = SegformerModel(config)
            self.forward = self.hug_forward
            self.partial_forward = self.hug_partial_forward
            self.n_blocks = n_blocks

    # def init_pretrained_weights(self, weights=None, channels_to_load=None):
    #     first_conv = 'encoder.patch_embeddings.0.proj.weight'
    #     weights = SegformerModel.from_pretrained(self.url).state_dict() if weights is None else weights
    #
    #     if channels_to_load is None:
    #         channels_to_load = slice(self.config.num_channels)
    #     else:
    #         channels_to_load = [CHANNEL_PRETRAIN[x] for x in channels_to_load]
    #
    #     if list(weights.keys())[0][:len("encoder.encoder")] == "encoder.encoder": # Remove extra encoder.
    #         weights = {k[len("encoder."):]: v for k, v in weights.items()}
    #     keys = weights.keys()
    #     fkeys = [k for k in keys if int(k.split('.')[2]) < self.n_blocks]
    #     weights = {k: weights[k] for k in fkeys}
    #
    #     num_to_load = len(channels_to_load) if isinstance(channels_to_load, list) else channels_to_load.stop
    #
    #     weights[first_conv] = weights[first_conv][:, channels_to_load]
    #     if num_to_load < self.config.num_channels:
    #         remaining = self.config.num_channels - num_to_load
    #         weights[first_conv] = torch.cat([
    #             weights[first_conv],
    #             self.encoder.state_dict()[first_conv][:, -remaining:]], dim=1)
    #
    #     self.encoder.load_state_dict(weights)

    def hug_forward(self, x):
        return self.encoder(x, output_hidden_states=True).hidden_states

    def base_partial_forward(self, x: Tensor, block_slice) -> Tensor:
        B = x.shape[0]

        outputs = ()
        for i in range(self.n_blocks)[block_slice]:
            x, H, W = getattr(self, f"patch_embed{i + 1}")(x)
            for blk in getattr(self, f"block{i + 1}"):
                x = blk(x, H, W)
            x = getattr(self, f"norm{i + 1}")(x).reshape(B, H, W, -1).permute(0, 3, 1, 2)
            outputs = outputs + (x,)
        return outputs

    def base_forward(self, x):
        return self.base_partial_forward(x, slice(self.n_blocks))

    def hug_partial_forward(self, pixel_values, block_slice):
        batch_size = pixel_values.shape[0]
        all_hidden_states = ()
        hidden_states = pixel_values
        output_attentions = False
        for idx, x in list(enumerate(zip(
                self.encoder.encoder.patch_embeddings,
                self.encoder.encoder.block,
                self.encoder.encoder.layer_norm
        )))[block_slice]:
            embedding_layer, block_layer, norm_layer = x
            # first, obtain patch embeddings
            hidden_states, height, width = embedding_layer(hidden_states)
            # second, send embeddings through blocks
            for i, blk in enumerate(block_layer):
                layer_outputs = blk(hidden_states, height, width, output_attentions)
                hidden_states = layer_outputs[0]
            # third, apply layer norm
            hidden_states = norm_layer(hidden_states)
            # fourth, optionally reshape back to (batch_size, num_channels, height, width)
            if idx != len(self.encoder.encoder.patch_embeddings) - 1 or (
                    idx == len(
                self.encoder.encoder.patch_embeddings) - 1 and self.encoder.encoder.config.reshape_last_stage
            ):
                hidden_states = hidden_states.reshape(batch_size, height, width, -1).permute(0, 3, 1, 2).contiguous()
            all_hidden_states = all_hidden_states + (hidden_states,)
        return all_hidden_states


class Block(nn.Module):
    def __init__(self, dim, head, sr_ratio=1, dpr=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, head, sr_ratio)
        self.drop_path = DropPath(dpr) if dpr > 0. else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = MLP(dim, int(dim * 4))

    def forward(self, x: Tensor, H, W) -> Tensor:
        x = x + self.drop_path(self.attn(self.norm1(x), H, W))
        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))
        return x


class Attention(nn.Module):
    def __init__(self, dim, head, sr_ratio):
        super().__init__()
        self.head = head
        self.sr_ratio = sr_ratio
        self.scale = (dim // head) ** -0.5
        self.q = nn.Linear(dim, dim)
        self.kv = nn.Linear(dim, dim * 2)
        self.proj = nn.Linear(dim, dim)

        if sr_ratio > 1:
            self.sr = nn.Conv2d(dim, dim, sr_ratio, sr_ratio)
            self.norm = nn.LayerNorm(dim)

    def forward(self, x: Tensor, H, W) -> Tensor:
        B, N, C = x.shape
        q = self.q(x).reshape(B, N, self.head, C // self.head).permute(0, 2, 1, 3)

        if self.sr_ratio > 1:
            x = x.permute(0, 2, 1).reshape(B, C, H, W)
            x = self.sr(x).reshape(B, C, -1).permute(0, 2, 1)
            x = self.norm(x)

        k, v = self.kv(x).reshape(B, -1, 2, self.head, C // self.head).permute(2, 0, 3, 1, 4)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        return x


class DropPath(nn.Module):
    def __init__(self, p: float = None):
        super().__init__()
        self.p = p

    def forward(self, x: Tensor) -> Tensor:
        if self.p == 0. or not self.training:
            return x
        kp = 1 - self.p
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = kp + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()  # binarize
        return x.div(kp) * random_tensor


class DWConv(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, groups=dim)

    def forward(self, x: Tensor, H, W) -> Tensor:
        B, _, C = x.shape
        x = x.transpose(1, 2).view(B, C, H, W)
        x = self.dwconv(x)
        return x.flatten(2).transpose(1, 2)


class MLP(nn.Module):
    def __init__(self, c1, c2):
        super().__init__()
        self.fc1 = nn.Linear(c1, c2)
        self.dwconv = DWConv(c2)
        self.fc2 = nn.Linear(c2, c1)

    def forward(self, x: Tensor, H, W) -> Tensor:
        return self.fc2(F.gelu(self.dwconv(self.fc1(x), H, W)))


class PatchEmbed(nn.Module):
    def __init__(self, c1=3, c2=32, patch_size=7, stride=4):
        super().__init__()
        self.proj = nn.Conv2d(c1, c2, patch_size, stride, patch_size // 2)  # padding=(ps[0]//2, ps[1]//2)
        self.norm = nn.LayerNorm(c2)

    def forward(self, x: Tensor) -> Tuple[Any, Any, Any]:
        x = self.proj(x)
        _, _, H, W = x.shape
        x = x.flatten(2).transpose(1, 2)
        x = self.norm(x)
        return x, H, W
